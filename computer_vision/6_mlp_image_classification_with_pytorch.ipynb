{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification with modern MLP models"
      ],
      "metadata": {
        "id": "zq_6H3B2UUYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This example implements three modern attention-free, multi-layer perceptron (MLP) based models for image\n",
        "classification, demonstrated on the CIFAR-100 dataset:\n",
        "\n",
        "1. The [MLP-Mixer](https://arxiv.org/abs/2105.01601) model, by Ilya Tolstikhin et al., based on two types of MLPs.\n",
        "3. The [FNet](https://arxiv.org/abs/2105.03824) model, by James Lee-Thorp et al., based on unparameterized\n",
        "Fourier Transform.\n",
        "2. The [gMLP](https://arxiv.org/abs/2105.08050) model, by Hanxiao Liu et al., based on MLP with gating.\n",
        "\n",
        "The purpose of the example is not to compare between these models, as they might perform differently on\n",
        "different datasets with well-tuned hyperparameters. Rather, it is to show simple implementations of their\n",
        "main building blocks."
      ],
      "metadata": {
        "id": "QLGvER-gUWYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "E97csvItUYBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JIEt6w6xTitI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "XhJe2ExMVeqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 100\n",
        "input_shape = (3, 32, 32)  # PyTorch uses (C, H, W) format\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # Normalize with CIFAR-100 stats\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f\"x_train shape: {train_dataset.data.shape} - y_train shape: {np.array(train_dataset.targets).shape}\")\n",
        "print(f\"x_test shape: {test_dataset.data.shape} - y_test shape: {np.array(test_dataset.targets).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWwWwcNUVe5i",
        "outputId": "ac51c1aa-f890-4a0f-8f59-8074f113d2a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000,)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--  -->"
      ],
      "metadata": {
        "id": "aqTVvWGTwwYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the hyperparameters"
      ],
      "metadata": {
        "id": "yf5ZdhMdXEAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 1  # Recommended num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of blocks.\n",
        "\n",
        "# Print hyperparameters\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2}\")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzWdRpJQVhO4",
        "outputId": "c4ba802f-1223-4b6a-9604-beb5edc661b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64\n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use data augmentation"
      ],
      "metadata": {
        "id": "JayWUOo2Xd2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and variance of the training data\n",
        "x_train_np = train_dataset.data / 255.0  # Normalize pixel values to [0, 1]\n",
        "mean = np.mean(x_train_np, axis=(0, 1, 2))  # Mean across channels\n",
        "std = np.std(x_train_np, axis=(0, 1, 2))  # Std across channels\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")\n",
        "\n",
        "data_augmentation = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),  # Resize images to the desired size\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "    transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),  # Random zoom\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean, std)  # Normalize with computed mean and std\n",
        "])\n",
        "\n",
        "# Apply data augmentation to the training dataset\n",
        "train_dataset = datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=data_augmentation  # Apply data augmentation\n",
        ")\n",
        "\n",
        "# For the test dataset, use only resizing and normalization (no augmentation)\n",
        "test_dataset = datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),  # Resize images to the desired size\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize(mean, std)  # Normalize with computed mean and std\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDlRzZkaXDV4",
        "outputId": "2cc89194-6c4f-45bd-a50c-fea2fcb42bb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: [0.50707516 0.48654887 0.44091784]\n",
            "Std: [0.26733429 0.25643846 0.27615047]\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement patch extraction as a layer"
      ],
      "metadata": {
        "id": "vVhziOIqXv-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(nn.Module):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Extracts patches from the input image tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of shape (B, num_patches, patch_dim), where\n",
        "                          num_patches = (H // patch_size) * (W // patch_size),\n",
        "                          patch_dim = C * patch_size * patch_size.\n",
        "        \"\"\"\n",
        "        # Extract patches using PyTorch's unfold operation\n",
        "        B, C, H, W = x.shape\n",
        "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        patches = patches.contiguous().view(B, C, -1, self.patch_size, self.patch_size)  # (B, C, num_patches, patch_size, patch_size)\n",
        "        patches = patches.permute(0, 2, 1, 3, 4)  # (B, num_patches, C, patch_size, patch_size)\n",
        "        patches = patches.reshape(B, -1, C * self.patch_size * self.patch_size)  # (B, num_patches, patch_dim)\n",
        "\n",
        "        return patches"
      ],
      "metadata": {
        "id": "c9rOiGC8XwKm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement position embedding as a layer"
      ],
      "metadata": {
        "id": "QizM0H-JX170"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, sequence_length, feature_size, initializer=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence_length (int): Length of the sequence.\n",
        "            feature_size (int): Size of the feature dimension.\n",
        "            initializer (callable): Initializer for the position embeddings. If None, uses default PyTorch initialization.\n",
        "        \"\"\"\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        # Initialize position embeddings\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(sequence_length, feature_size))\n",
        "        if initializer is not None:\n",
        "            initializer(self.position_embeddings)\n",
        "\n",
        "    def forward(self, inputs, start_index=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input tensor of shape (B, sequence_length, feature_size).\n",
        "            start_index (int): Index from which to start slicing the position embeddings.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (B, sequence_length, feature_size).\n",
        "        \"\"\"\n",
        "        B, seq_len, feature_size = inputs.shape\n",
        "\n",
        "        # Trim position embeddings to match the input sequence length\n",
        "        position_embeddings = self.position_embeddings[start_index : start_index + seq_len, :]\n",
        "\n",
        "        # Broadcast position embeddings to match the input shape\n",
        "        position_embeddings = position_embeddings.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        return inputs + position_embeddings"
      ],
      "metadata": {
        "id": "1dPMYb5zXyz5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a classification model\n",
        "\n",
        "We implement a method that builds a classifier given the processing blocks."
      ],
      "metadata": {
        "id": "ExasIgq5YKqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, blocks, patch_size, embedding_dim, num_patches, num_classes, dropout_rate, positional_encoding=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            blocks (nn.Module): A PyTorch module that processes the input tensor.\n",
        "            patch_size (int): Size of the patches.\n",
        "            embedding_dim (int): Dimensionality of the patch embeddings.\n",
        "            num_patches (int): Number of patches per image.\n",
        "            num_classes (int): Number of output classes.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "            positional_encoding (bool): Whether to add positional encoding.\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_patches = num_patches\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.positional_encoding = positional_encoding\n",
        "\n",
        "        # Layers\n",
        "        self.patches_layer = Patches(patch_size)\n",
        "        self.dense = nn.Linear((patch_size ** 2) * 3, embedding_dim)\n",
        "        if positional_encoding:\n",
        "            self.position_embedding = PositionEmbedding(num_patches, embedding_dim)\n",
        "        self.blocks = blocks\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create patches\n",
        "        patches = self.patches_layer(x)  # (B, num_patches, patch_dim)\n",
        "        # Encode patches\n",
        "        x = self.dense(patches)  # (B, num_patches, embedding_dim)\n",
        "        # Add positional encoding\n",
        "        if self.positional_encoding:\n",
        "            x = x + self.position_embedding(x)  # (B, num_patches, embedding_dim)\n",
        "        # Process through blocks\n",
        "        x = self.blocks(x)  # (B, num_patches, embedding_dim)\n",
        "        # Global average pooling\n",
        "        x = x.permute(0, 2, 1)  # (B, embedding_dim, num_patches)\n",
        "        x = self.global_pool(x)  # (B, embedding_dim, 1)\n",
        "        x = x.squeeze(2)  # (B, embedding_dim)\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "        # Compute logits\n",
        "        logits = self.classifier(x)  # (B, num_classes)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "6k1t6jk3YEcp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an experiment\n",
        "\n",
        "We implement a utility function to compile, train, and evaluate a given model."
      ],
      "metadata": {
        "id": "HH_FlUq6ZGFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, learning_rate, train_dataset, test_dataset, batch_size, num_epochs, weight_decay):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to train.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        train_dataset (torch.utils.data.Dataset): Training dataset.\n",
        "        test_dataset (torch.utils.data.Dataset): Test dataset.\n",
        "        batch_size (int): Batch size for training and evaluation.\n",
        "        num_epochs (int): Number of epochs to train.\n",
        "        weight_decay (float): Weight decay for the optimizer.\n",
        "    \"\"\"\n",
        "    # Set device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Split training data into training and validation sets\n",
        "    train_size = int(0.9 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create DataLoader objects\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Define optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Define learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_acc += accuracy_score(labels.cpu(), predicted.cpu())\n",
        "\n",
        "        # Average losses and accuracy\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc /= len(val_loader)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, \"\n",
        "              f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model.eval()\n",
        "    test_acc = 0.0\n",
        "    test_top5_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate top-1 accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_acc += accuracy_score(labels.cpu(), predicted.cpu())\n",
        "\n",
        "            # Calculate top-5 accuracy\n",
        "            _, top5_predicted = torch.topk(outputs, 5, dim=1)\n",
        "            top5_correct = torch.sum(top5_predicted == labels.view(-1, 1))\n",
        "            test_top5_acc += top5_correct.item() / labels.size(0)\n",
        "\n",
        "    # Average test accuracy\n",
        "    test_acc /= len(test_loader)\n",
        "    test_top5_acc /= len(test_loader)\n",
        "\n",
        "    # Print test results\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "    print(f\"Test Top-5 Accuracy: {test_top5_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "8iFXz70_ZGUw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The MLP-Mixer model\n",
        "\n",
        "The MLP-Mixer is an architecture based exclusively on\n",
        "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
        "\n",
        "1. One applied independently to image patches, which mixes the per-location features.\n",
        "2. The other applied across patches (along channels), which mixes spatial information.\n",
        "\n",
        "This is similar to a [depthwise separable convolution based model](https://arxiv.org/abs/1610.02357)\n",
        "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
        "instead of batch normalization."
      ],
      "metadata": {
        "id": "eJhgiawMaTwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the MLP-Mixer module"
      ],
      "metadata": {
        "id": "jwgEFJYTaVjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPMixerLayer(nn.Module):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_patches (int): Number of patches.\n",
        "            hidden_units (int): Number of hidden units in the MLP.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(MLPMixerLayer, self).__init__()\n",
        "\n",
        "        # MLP applied across patches (mixing spatial information)\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(num_patches, num_patches),  # First dense layer\n",
        "            nn.GELU(),  # GELU activation\n",
        "            nn.Linear(num_patches, num_patches),  # Second dense layer\n",
        "            nn.Dropout(dropout_rate)  # Dropout\n",
        "        )\n",
        "\n",
        "        # MLP applied across channels (mixing per-location features)\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Linear(hidden_units, hidden_units),  # First dense layer\n",
        "            nn.GELU(),  # GELU activation\n",
        "            nn.Linear(hidden_units, hidden_units),  # Second dense layer\n",
        "            nn.Dropout(dropout_rate)  # Dropout\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.normalize = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input tensor of shape (B, num_patches, hidden_units).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (B, num_patches, hidden_units).\n",
        "        \"\"\"\n",
        "        # Apply layer normalization\n",
        "        x = self.normalize(inputs)  # (B, num_patches, hidden_units)\n",
        "\n",
        "        # Transpose inputs to mix across patches\n",
        "        x_channels = x.transpose(1, 2)  # (B, hidden_units, num_patches)\n",
        "\n",
        "        # Apply mlp1 on each channel independently\n",
        "        mlp1_outputs = self.mlp1(x_channels)  # (B, hidden_units, num_patches)\n",
        "\n",
        "        # Transpose back to original shape\n",
        "        mlp1_outputs = mlp1_outputs.transpose(1, 2)  # (B, num_patches, hidden_units)\n",
        "\n",
        "        # Add skip connection\n",
        "        x = mlp1_outputs + inputs  # (B, num_patches, hidden_units)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x_patches = self.normalize(x)  # (B, num_patches, hidden_units)\n",
        "\n",
        "        # Apply mlp2 on each patch independently\n",
        "        mlp2_outputs = self.mlp2(x_patches)  # (B, num_patches, hidden_units)\n",
        "\n",
        "        # Add skip connection\n",
        "        x = x + mlp2_outputs  # (B, num_patches, hidden_units)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "hmiIgiFNaUCi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build, train, and evaluate the MLP-Mixer model"
      ],
      "metadata": {
        "id": "yN2TfZFmadYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the MLP-Mixer blocks\n",
        "mlpmixer_blocks = nn.Sequential(\n",
        "    *[MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "\n",
        "# Create the classifier model\n",
        "mlpmixer_classifier = Classifier(\n",
        "    blocks=mlpmixer_blocks,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_patches=num_patches,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=dropout_rate,\n",
        "    positional_encoding=True\n",
        ")\n",
        "\n",
        "# Define learning rate\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Run the experiment\n",
        "run_experiment(\n",
        "    model=mlpmixer_classifier,\n",
        "    learning_rate=learning_rate,\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    weight_decay=weight_decay\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMTlh0JIadl4",
        "outputId": "fb61fd78-a60a-4bac-968e-2f3fdbbd504b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 3.9079, Val Loss: 3.6088, Val Acc: 0.1701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-43ecef6f0199>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 17.20%\n",
            "Test Top-5 Accuracy: 42.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MLP-Mixer model tends to have much less number of parameters compared\n",
        "to convolutional and transformer-based models, which leads to less training and\n",
        "serving computational cost.\n",
        "\n",
        "As mentioned in the [MLP-Mixer](https://arxiv.org/abs/2105.01601) paper,\n",
        "when pre-trained on large datasets, or with modern regularization schemes,\n",
        "the MLP-Mixer attains competitive scores to state-of-the-art models.\n",
        "You can obtain better results by increasing the embedding dimensions,\n",
        "increasing the number of mixer blocks, and training the model for longer.\n",
        "You may also try to increase the size of the input images and use different patch sizes."
      ],
      "metadata": {
        "id": "50fKV94HDhUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The FNet model\n",
        "\n",
        "The FNet uses a similar block to the Transformer block. However, FNet replaces the self-attention layer\n",
        "in the Transformer block with a parameter-free 2D Fourier transformation layer:\n",
        "\n",
        "1. One 1D Fourier Transform is applied along the patches.\n",
        "2. One 1D Fourier Transform is applied along the channels."
      ],
      "metadata": {
        "id": "85-hq59rDtPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the FNet module"
      ],
      "metadata": {
        "id": "qmc0plPXDvev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FNetLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout_rate):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_dim (int): Dimensionality of the input and output features.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(FNetLayer, self).__init__()\n",
        "\n",
        "        # Feedforward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),  # First dense layer\n",
        "            nn.GELU(),  # GELU activation\n",
        "            nn.Dropout(dropout_rate),  # Dropout\n",
        "            nn.Linear(embedding_dim, embedding_dim)  # Second dense layer\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.normalize1 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
        "        self.normalize2 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input tensor of shape (B, num_patches, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (B, num_patches, embedding_dim).\n",
        "        \"\"\"\n",
        "        # Apply 2D Fourier Transform\n",
        "        x = torch.fft.fft2(inputs).real  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Add skip connection\n",
        "        x = x + inputs  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.normalize1(x)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply feedforward network\n",
        "        x_ffn = self.ffn(x)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Add skip connection\n",
        "        x = x + x_ffn  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.normalize2(x)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "KQjHsr3PDhjV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build, train, and evaluate the FNet model"
      ],
      "metadata": {
        "id": "Z81_zAk0WDwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the FNet blocks\n",
        "fnet_blocks = nn.Sequential(\n",
        "    *[FNetLayer(embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "\n",
        "# Create the classifier model\n",
        "fnet_classifier = Classifier(\n",
        "    blocks=fnet_blocks,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_patches=num_patches,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=dropout_rate,\n",
        "    positional_encoding=True\n",
        ")\n",
        "\n",
        "# Define learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Run the experiment\n",
        "run_experiment(\n",
        "    model=fnet_classifier,\n",
        "    learning_rate=learning_rate,\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    weight_decay=weight_decay\n",
        ")"
      ],
      "metadata": {
        "id": "0_O6hBMQWD-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7addd158-3f5f-479f-84eb-f40d65e82dab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 4.1169, Val Loss: 3.8115, Val Acc: 0.1178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-43ecef6f0199>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 11.55%\n",
            "Test Top-5 Accuracy: 32.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the [FNet](https://arxiv.org/abs/2105.03824) paper,\n",
        "better results can be achieved by increasing the embedding dimensions,\n",
        "increasing the number of FNet blocks, and training the model for longer.\n",
        "You may also try to increase the size of the input images and use different patch sizes.\n",
        "The FNet scales very efficiently to long inputs, runs much faster than attention-based\n",
        "Transformer models, and produces competitive accuracy results."
      ],
      "metadata": {
        "id": "79Ofgz8jYaUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The gMLP model\n",
        "\n",
        "The gMLP is a MLP architecture that features a Spatial Gating Unit (SGU).\n",
        "The SGU enables cross-patch interactions across the spatial (channel) dimension, by:\n",
        "\n",
        "1. Transforming the input spatially by applying linear projection across patches (along channels).\n",
        "2. Applying element-wise multiplication of the input and its spatial transformation."
      ],
      "metadata": {
        "id": "zNc0GtDPYbvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the gMLP module"
      ],
      "metadata": {
        "id": "qI4bhxzxYeXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class gMLPLayer(nn.Module):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_patches (int): Number of patches.\n",
        "            embedding_dim (int): Dimensionality of the input and output features.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(gMLPLayer, self).__init__()\n",
        "\n",
        "        # Channel projection 1\n",
        "        self.channel_projection1 = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim * 2),  # First dense layer\n",
        "            nn.GELU(),  # GELU activation\n",
        "            nn.Dropout(dropout_rate)  # Dropout\n",
        "        )\n",
        "\n",
        "        # Channel projection 2\n",
        "        self.channel_projection2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # Spatial projection\n",
        "        self.spatial_projection = nn.Linear(num_patches, num_patches)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.normalize1 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
        "        self.normalize2 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
        "\n",
        "    def spatial_gating_unit(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (B, num_patches, embedding_dim * 2).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (B, num_patches, embedding_dim).\n",
        "        \"\"\"\n",
        "        # Split x along the channel dimension\n",
        "        u, v = torch.split(x, x.size(2) // 2, dim=2)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        v = self.normalize2(v)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply spatial projection\n",
        "        v = v.transpose(1, 2)  # (B, embedding_dim, num_patches)\n",
        "        v_projected = self.spatial_projection(v)  # (B, embedding_dim, num_patches)\n",
        "        v_projected = v_projected.transpose(1, 2)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply element-wise multiplication\n",
        "        return u * v_projected  # (B, num_patches, embedding_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input tensor of shape (B, num_patches, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (B, num_patches, embedding_dim).\n",
        "        \"\"\"\n",
        "        # Apply layer normalization\n",
        "        x = self.normalize1(inputs)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply the first channel projection\n",
        "        x_projected = self.channel_projection1(x)  # (B, num_patches, embedding_dim * 2)\n",
        "\n",
        "        # Apply the spatial gating unit\n",
        "        x_spatial = self.spatial_gating_unit(x_projected)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply the second channel projection\n",
        "        x_projected = self.channel_projection2(x_spatial)  # (B, num_patches, embedding_dim)\n",
        "\n",
        "        # Add skip connection\n",
        "        return inputs + x_projected  # (B, num_patches, embedding_dim)"
      ],
      "metadata": {
        "id": "3usVVoPlYalU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build, train, and evaluate the gMLP model"
      ],
      "metadata": {
        "id": "Ga4Z4EEK6_Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the gMLP blocks\n",
        "gmlp_blocks = nn.Sequential(\n",
        "    *[gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "\n",
        "# Create the classifier model\n",
        "gmlp_classifier = Classifier(\n",
        "    blocks=gmlp_blocks,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_patches=num_patches,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=dropout_rate,\n",
        "    positional_encoding=True\n",
        ")\n",
        "\n",
        "# Define learning rate\n",
        "learning_rate = 0.003\n",
        "\n",
        "# Run the experiment\n",
        "run_experiment(\n",
        "    model=gmlp_classifier,\n",
        "    learning_rate=learning_rate,\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    weight_decay=weight_decay\n",
        ")"
      ],
      "metadata": {
        "id": "sdbsltR-7Brx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f550b89-be5d-4126-db43-6e8895398082"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 3.9089, Val Loss: 3.6911, Val Acc: 0.1506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-43ecef6f0199>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 17.46%\n",
            "Test Top-5 Accuracy: 42.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the [gMLP](https://arxiv.org/abs/2105.08050) paper,\n",
        "better results can be achieved by increasing the embedding dimensions,\n",
        "increasing the number of gMLP blocks, and training the model for longer.\n",
        "You may also try to increase the size of the input images and use different patch sizes.\n",
        "Note that, the paper used advanced regularization strategies, such as MixUp and CutMix,\n",
        "as well as AutoAugment."
      ],
      "metadata": {
        "id": "yYSceSdP7J_d"
      }
    }
  ]
}